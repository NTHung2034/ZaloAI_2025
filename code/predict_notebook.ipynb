{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp_5vnP3i8KU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import glob\n",
        "\n",
        "\n",
        "# TEST_DATA_DIR\n",
        "TEST_DATA_DIR = \"/src/data/test/samples\"\n",
        "\n",
        "# MODEL_PATH\n",
        "MODEL_PATH = \"/src/weight/best.pt\"\n",
        "\n",
        "OUTPUT_JSON = 'result/jupyter_submission.json'\n",
        "OUTPUT_TIME_CSV = 'result/jupyter_time_submission.csv'\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(2024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv2prgTjkHpz",
        "outputId": "da9f544f-1121-4bb5-cf09-2cedb14dc8fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Loading YOLO11 model from /content/drive/MyDrive/ZALO_AI/final_zalo_full/train2/weights/best.pt...\n",
            "Warming up model...\n",
            "Model loaded and warmed up successfully!\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# --- LOAD MODEL ---\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "\n",
        "try:\n",
        "    model = YOLO(MODEL_PATH)\n",
        "    model.predict(source=np.zeros((640, 640, 3), dtype=np.uint8), verbose=False, imgsz=640)\n",
        "\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl6DtoPSkoie",
        "outputId": "37204b0e-96f3-4938-c1b0-d85be3842806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6 test cases folders.\n",
            "Example case: /content/drive/MyDrive/ZALO_AI/public_test/samples/BlackBox_0\n",
            " - Video path check: True\n"
          ]
        }
      ],
      "source": [
        "# Test Case\n",
        "# TEST_DATA_DIR\n",
        "subfolders = sorted(glob.glob(os.path.join(TEST_DATA_DIR, '*')))\n",
        "test_cases = [f for f in subfolders if os.path.isdir(f)]\n",
        "\n",
        "print(f\"Found {len(test_cases)} test cases folders.\")\n",
        "if len(test_cases) > 0:\n",
        "    print(f\"Example case: {test_cases[0]}\")\n",
        "\n",
        "    example_video = os.path.join(test_cases[0], 'drone_video.mp4')\n",
        "    print(f\" - Video path check: {os.path.exists(example_video)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHQzFDWGlUvT",
        "outputId": "dee13937-7d69-4267-9a10-92bbf76989bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting inference on Test Cases...\n",
            "WARNING ⚠️ \n",
            "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "Processed BlackBox_0 in 153216ms\n",
            "WARNING ⚠️ \n",
            "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Inference Loop\n",
        "all_predicted_time = []\n",
        "all_results = []\n",
        "\n",
        "print(\"Starting inference on Test Cases...\")\n",
        "\n",
        "for case_folder in test_cases:\n",
        "    case_id = os.path.basename(case_folder)\n",
        "\n",
        "    video_path = os.path.join(case_folder, 'drone_video.mp4')\n",
        "    images_dir = os.path.join(case_folder, 'object_images')\n",
        "\n",
        "    # --- TÍNH GIỜ ---\n",
        "    t1 = time.time()\n",
        "\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Warning: {video_path} not found!\")\n",
        "        continue\n",
        "\n",
        "    # --- PREDICT ---\n",
        "    results = model.predict(\n",
        "        source=video_path,\n",
        "        verbose=False,\n",
        "        conf=0.25,\n",
        "        save=False,\n",
        "        device=0 if torch.cuda.is_available() else 'cpu'\n",
        "    )\n",
        "\n",
        "    frame_results = []\n",
        "    for r in results:\n",
        "        # r là kết quả của 1 frame\n",
        "        boxes = r.boxes.xywh.tolist() if r.boxes else []\n",
        "        cls = r.boxes.cls.tolist() if r.boxes else []\n",
        "\n",
        "        if boxes:\n",
        "            frame_results.append({\n",
        "                \"frame_idx\": len(frame_results),\n",
        "                \"boxes\": boxes,\n",
        "                \"classes\": cls\n",
        "            })\n",
        "\n",
        "\n",
        "    final_answer = frame_results\n",
        "\n",
        "    # --- KẾT THÚC TÍNH GIỜ ---\n",
        "    t2 = time.time()\n",
        "\n",
        "    predicted_time = int((t2 - t1) * 1000)\n",
        "\n",
        "\n",
        "    all_predicted_time.append({\n",
        "        \"id\": case_id,\n",
        "        \"answer\": \"predict_done\",\n",
        "        \"time\": predicted_time\n",
        "    })\n",
        "\n",
        "    # Lưu kết quả chi tiết (cho file submission.json)\n",
        "    all_results.append({\n",
        "        \"id\": case_id,\n",
        "        \"prediction\": final_answer\n",
        "    })\n",
        "\n",
        "    print(f\"Processed {case_id} in {predicted_time}ms\")\n",
        "\n",
        "print(\"Inference completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUxuxlNSl8GS"
      },
      "outputs": [],
      "source": [
        "# jupyter_time_submission.csv\n",
        "# Yêu cầu format 3 cột: id, answer, time\n",
        "with open(OUTPUT_TIME_CSV, mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['id', 'answer', 'time']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for item in all_predicted_time:\n",
        "        writer.writerow(item)\n",
        "\n",
        "print(f\"Saved time submission to {OUTPUT_TIME_CSV}\")\n",
        "\n",
        "# jupyter_submission.json\n",
        "# Nội dung tương tự submission.json\n",
        "with open(OUTPUT_JSON, 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(all_results, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Saved result submission to {OUTPUT_JSON}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
