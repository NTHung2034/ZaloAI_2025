{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T06:46:00.950990Z",
     "iopub.status.busy": "2025-11-18T06:46:00.950705Z",
     "iopub.status.idle": "2025-11-18T06:46:09.683250Z",
     "shell.execute_reply": "2025-11-18T06:46:09.682477Z",
     "shell.execute_reply.started": "2025-11-18T06:46:00.950969Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.228)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.1.3)\n",
      "Requirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.25.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.3)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.12.4)\n",
      "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics\n",
    "!pip install albumentations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T06:46:09.684860Z",
     "iopub.status.busy": "2025-11-18T06:46:09.684565Z",
     "iopub.status.idle": "2025-11-18T06:46:09.694872Z",
     "shell.execute_reply": "2025-11-18T06:46:09.694122Z",
     "shell.execute_reply.started": "2025-11-18T06:46:09.684841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(img_path, img_size=640):\n",
    "    \"\"\"Load and preprocess image\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            return None\n",
    "        \n",
    "        if len(img.shape) != 3 or img.shape[2] != 3:\n",
    "            return None\n",
    "        \n",
    "        # Convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize\n",
    "        img = cv2.resize(img, (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Normalize directly with torch\n",
    "        img_tensor = torch.from_numpy(img).float().div(255.0).permute(2, 0, 1)\n",
    "        \n",
    "        return img_tensor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_frame(video_path, frame_num, img_size=640):\n",
    "    \"\"\"Extract specific frame from video\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret or frame is None:\n",
    "            return None, None\n",
    "        \n",
    "        if len(frame.shape) != 3 or frame.shape[2] != 3:\n",
    "            return None, None\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w = frame.shape[:2]\n",
    "        \n",
    "        # Resize\n",
    "        frame = cv2.resize(frame, (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Normalize directly with torch\n",
    "        frame_tensor = torch.from_numpy(frame).float().div(255.0).permute(2, 0, 1)\n",
    "        \n",
    "        return frame_tensor, (w, h)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting frame: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def convert_bbox_to_yolo(bbox, orig_w, orig_h, img_size=640):\n",
    "    \"\"\"Convert bbox to YOLO format (normalized)\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    \n",
    "    scale_x = img_size / orig_w\n",
    "    scale_y = img_size / orig_h\n",
    "    \n",
    "    x1 = x1 * scale_x\n",
    "    y1 = y1 * scale_y\n",
    "    x2 = x2 * scale_x\n",
    "    y2 = y2 * scale_y\n",
    "    \n",
    "    x_center = (x1 + x2) / 2.0 / img_size\n",
    "    y_center = (y1 + y2) / 2.0 / img_size\n",
    "    width = (x2 - x1) / img_size\n",
    "    height = (y2 - y1) / img_size\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    x_center = np.clip(x_center, 0, 1)\n",
    "    y_center = np.clip(y_center, 0, 1)\n",
    "    width = np.clip(width, 0, 1)\n",
    "    height = np.clip(height, 0, 1)\n",
    "    \n",
    "    return torch.tensor([x_center, y_center, width, height])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T06:46:09.696140Z",
     "iopub.status.busy": "2025-11-18T06:46:09.695851Z",
     "iopub.status.idle": "2025-11-18T06:46:09.735670Z",
     "shell.execute_reply": "2025-11-18T06:46:09.734878Z",
     "shell.execute_reply.started": "2025-11-18T06:46:09.696114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì PrototypicalTrainer ƒê·∫¶Y ƒê·ª¶ CH·ª®C NƒÇNG:\n",
      "  1. ‚úì Multi-GPU support\n",
      "  2. ‚úì Mixed precision training\n",
      "  3. ‚úì Gradient clipping\n",
      "  4. ‚úì Learning rate scheduling\n",
      "  5. ‚úì Save/Load checkpoint ƒê·∫¶Y ƒê·ª¶\n",
      "  6. ‚úì Training history tracking\n",
      "  7. ‚úì Memory optimization\n",
      "  \n",
      "BACKBONE ƒê∆Ø·ª¢C L∆ØU TRONG:\n",
      "- checkpoint['model_state_dict'] ch·ª©a T·∫§T C·∫¢ weights (backbone + heads)\n",
      "- Backbone v·∫´n frozen (kh√¥ng update), nh∆∞ng ƒê∆Ø·ª¢C L∆ØU HO√ÄN CH·ªàNH\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prototypical Networks: leaner metric-learning approach for few-shot detection\n",
    "that trains embeddings and classifies by distance to a prototype; simpler than\n",
    "MAML for this use case.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "class PrototypicalYOLO(nn.Module):\n",
    "    \"\"\"\n",
    "    Few-shot detector that reuses a frozen YOLO backbone and learns a\n",
    "    prototypical embedding/detection head.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_path='yolo11n.pt', device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Feature extractor\n",
    "        from ultralytics import YOLO\n",
    "        yolo = YOLO(base_model_path)\n",
    "        self.backbone = yolo.model.to(device)\n",
    "        \n",
    "        # Freeze backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Embedding head (project features to metric space)\n",
    "        self.embedding_head = nn.Sequential(\n",
    "            nn.Conv2d(144, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, 1),  # Embedding dimension = 128\n",
    "        ).to(device)\n",
    "        \n",
    "        # Detection head (gi·ªëng c≈©)\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 4, 1)  # 4 coords\n",
    "        ).to(device)\n",
    "        \n",
    "        self.obj_head = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, 1)  # Objectness\n",
    "        ).to(device)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Extract backbone features.\"\"\"\n",
    "        self.backbone.train()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "            if isinstance(features, (list, tuple)):\n",
    "                features = features[0]\n",
    "        return features\n",
    "        \n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: [B, 3, H, W]\n",
    "            return_embeddings: N·∫øu True, tr·∫£ v·ªÅ embeddings thay v√¨ detections\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: [B, 128, H/32, W/32] ho·∫∑c\n",
    "            (bbox, obj): Predictions\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        features = self.extract_features(x)  # [B, 144, H/32, W/32]\n",
    "        \n",
    "        # Get embeddings\n",
    "        embeddings = self.embedding_head(features)  # [B, 128, H/32, W/32]\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return embeddings\n",
    "        \n",
    "        # Detection\n",
    "        bbox = self.bbox_head(embeddings)\n",
    "        obj = self.obj_head(embeddings)\n",
    "        \n",
    "        return bbox, obj\n",
    "    \n",
    "    def compute_prototype(self, support_imgs, support_bboxes):\n",
    "        \"\"\"\n",
    "        Build a class prototype from the support set.\n",
    "        \n",
    "        Args:\n",
    "            support_imgs: [N, 3, H, W]\n",
    "            support_bboxes: [N, 4] normalized xywh\n",
    "        \n",
    "        Returns:\n",
    "            prototype: [128] embedding vector for the target object\n",
    "        \"\"\"\n",
    "        embeddings = self.forward(support_imgs, return_embeddings=True)\n",
    "        # [N, 128, H', W']\n",
    "        \n",
    "        # RoI pooling: L·∫•y embedding ·ªü v√πng bbox\n",
    "        N, C, H, W = embeddings.shape\n",
    "        \n",
    "        roi_embeddings = []\n",
    "        for i in range(N):\n",
    "            x_c, y_c, w, h = support_bboxes[i]\n",
    "            \n",
    "            # Convert to grid coords\n",
    "            x_c = int(x_c * W)\n",
    "            y_c = int(y_c * H)\n",
    "            w = max(1, int(w * W))\n",
    "            h = max(1, int(h * H))\n",
    "            \n",
    "            x1 = max(0, x_c - w//2)\n",
    "            y1 = max(0, y_c - h//2)\n",
    "            x2 = min(W, x_c + w//2)\n",
    "            y2 = min(H, y_c + h//2)\n",
    "            \n",
    "            # Average pooling trong RoI\n",
    "            roi = embeddings[i, :, y1:y2, x1:x2]  # [128, h, w]\n",
    "            roi_feat = roi.mean(dim=[1, 2])  # [128]\n",
    "            roi_embeddings.append(roi_feat)\n",
    "        \n",
    "        # Prototype is the mean of support embeddings\n",
    "        prototype = torch.stack(roi_embeddings).mean(dim=0)  # [128]\n",
    "        \n",
    "        return prototype\n",
    "    \n",
    "    def compute_loss_with_prototype(self, query_imgs, query_targets, prototype):\n",
    "        \"\"\"\n",
    "        Loss that ties detections to a class prototype.\n",
    "        \n",
    "        Args:\n",
    "            query_imgs: [B, 3, H, W]\n",
    "            query_targets: [B, 4]\n",
    "            prototype: [128] target embedding\n",
    "        \n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        bbox_pred, obj_pred = self.forward(query_imgs)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embeddings = self.forward(query_imgs, return_embeddings=True)\n",
    "        # [B, 128, H', W']\n",
    "        \n",
    "        B, C, H, W = embeddings.shape\n",
    "        \n",
    "        # Reshape\n",
    "        bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(B, -1, 4)\n",
    "        obj_pred = obj_pred.permute(0, 2, 3, 1).reshape(B, -1)\n",
    "        embeddings = embeddings.permute(0, 2, 3, 1).reshape(B, -1, C)\n",
    "        \n",
    "        # Loss 1: embedding similarity to the prototype\n",
    "        prototype_expanded = prototype.unsqueeze(0).unsqueeze(0)  # [1, 1, 128]\n",
    "        \n",
    "        distances = torch.norm(\n",
    "            embeddings - prototype_expanded, \n",
    "            dim=2\n",
    "        )  # [B, N]\n",
    "        \n",
    "        # Pick the anchor with highest IoU as the positive\n",
    "        def compute_anchor_iou(pred_boxes, target_boxes):\n",
    "            \"\"\"Compute IoU between predictions and targets.\"\"\"\n",
    "            def xywh2xyxy(boxes):\n",
    "                x, y, w, h = boxes[..., 0], boxes[..., 1], boxes[..., 2], boxes[..., 3]\n",
    "                x1 = x - w / 2\n",
    "                y1 = y - h / 2\n",
    "                x2 = x + w / 2\n",
    "                y2 = y + h / 2\n",
    "                return torch.stack([x1, y1, x2, y2], dim=-1)\n",
    "            \n",
    "            pred_xyxy = xywh2xyxy(pred_boxes)\n",
    "            target_xyxy = xywh2xyxy(target_boxes.unsqueeze(1))\n",
    "            \n",
    "            x1 = torch.max(pred_xyxy[..., 0], target_xyxy[..., 0])\n",
    "            y1 = torch.max(pred_xyxy[..., 1], target_xyxy[..., 1])\n",
    "            x2 = torch.min(pred_xyxy[..., 2], target_xyxy[..., 2])\n",
    "            y2 = torch.min(pred_xyxy[..., 3], target_xyxy[..., 3])\n",
    "            \n",
    "            inter = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "            pred_area = (pred_xyxy[..., 2] - pred_xyxy[..., 0]) * (pred_xyxy[..., 3] - pred_xyxy[..., 1])\n",
    "            target_area = (target_xyxy[..., 2] - target_xyxy[..., 0]) * (target_xyxy[..., 3] - target_xyxy[..., 1])\n",
    "            union = pred_area + target_area - inter + 1e-7\n",
    "            \n",
    "            return inter / union\n",
    "        \n",
    "        iou = compute_anchor_iou(bbox_pred, query_targets)  # [B, N]\n",
    "        best_anchor = iou.argmax(dim=1)  # [B]\n",
    "        \n",
    "        # Contrastive loss: K√©o best anchor g·∫ßn prototype, ƒë·∫©y c√°c anchor kh√°c xa\n",
    "        positive_distances = distances[torch.arange(B), best_anchor]\n",
    "        \n",
    "        # Negatives: all other anchors\n",
    "        negative_mask = torch.ones_like(distances, dtype=torch.bool)\n",
    "        negative_mask[torch.arange(B), best_anchor] = False\n",
    "        negative_distances = distances[negative_mask].view(B, -1)\n",
    "        \n",
    "        # Triplet-style margin loss\n",
    "        margin = 0.5\n",
    "        contrastive_loss = F.relu(\n",
    "            positive_distances.unsqueeze(1) - negative_distances + margin\n",
    "        ).mean()\n",
    "        \n",
    "        # Loss 2: bbox regression\n",
    "        best_bbox = bbox_pred[torch.arange(B), best_anchor]\n",
    "        bbox_loss = F.smooth_l1_loss(best_bbox, query_targets)\n",
    "        \n",
    "        # Loss 3: objectness\n",
    "        obj_targets = torch.zeros_like(obj_pred)\n",
    "        obj_targets[torch.arange(B), best_anchor] = 1.0\n",
    "        obj_loss = F.binary_cross_entropy_with_logits(obj_pred, obj_targets)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (\n",
    "            contrastive_loss * 2.0 +\n",
    "            bbox_loss * 5.0 +\n",
    "            obj_loss * 1.0\n",
    "        )\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# TRAINING LOOP - PHI√äN B·∫¢N ƒê·∫¶Y ƒê·ª¶\n",
    "# ==========================================\n",
    "\n",
    "class PrototypicalTrainer:\n",
    "    \"\"\"\n",
    "    End-to-end trainer for the prototypical detector, with multi-GPU,\n",
    "    AMP, checkpointing, and tracking utilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, cfg):\n",
    "        self.model = model\n",
    "        self.cfg = cfg\n",
    "        self.device = cfg.DEVICE\n",
    "        \n",
    "        # Multi-GPU support\n",
    "        if cfg.USE_MULTI_GPU and torch.cuda.device_count() > 1:\n",
    "            print(f\"‚úì Using {torch.cuda.device_count()} GPUs\")\n",
    "            self.model = nn.DataParallel(model)\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Get model for parameter access\n",
    "        self.base_model = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        \n",
    "        # Optimizer: only train embedding and detection heads\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': self.base_model.embedding_head.parameters(), 'lr': cfg.META_LR},\n",
    "            {'params': self.base_model.bbox_head.parameters(), 'lr': cfg.META_LR},\n",
    "            {'params': self.base_model.obj_head.parameters(), 'lr': cfg.META_LR},\n",
    "        ], weight_decay=0.01)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=cfg.META_EPOCHS\n",
    "        )\n",
    "        \n",
    "        # Mixed precision training\n",
    "        self.scaler = torch.amp.GradScaler('cuda') if cfg.MIXED_PRECISION else None\n",
    "        \n",
    "        # Tracking metrics\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def train_step(self, task_batch):\n",
    "        \"\"\"\n",
    "        One training step over a batch of few-shot tasks.\n",
    "        \n",
    "        task_batch: List of {\n",
    "            'support': [N_support, 3, H, W],\n",
    "            'query': [N_query, 3, H, W],\n",
    "            'support_targets': [N_support, 4],\n",
    "            'query_targets': [N_query, 4]\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        task_count = 0\n",
    "        \n",
    "        for task in task_batch:\n",
    "            support = task['support'].to(self.device)\n",
    "            query = task['query'].to(self.device)\n",
    "            support_targets = task['support_targets'].to(self.device)\n",
    "            query_targets = task['query_targets'].to(self.device)\n",
    "            \n",
    "            # Mixed precision context\n",
    "            with torch.amp.autocast('cuda', enabled=self.cfg.MIXED_PRECISION):\n",
    "                # 1. Compute prototype\n",
    "                prototype = self.base_model.compute_prototype(support, support_targets)\n",
    "                \n",
    "                # 2. Compute loss on query set\n",
    "                loss = self.base_model.compute_loss_with_prototype(\n",
    "                    query, query_targets, prototype\n",
    "                )\n",
    "            \n",
    "            total_loss += loss\n",
    "            task_count += 1\n",
    "        \n",
    "        # Backward\n",
    "        if task_count > 0:\n",
    "            total_loss = total_loss / task_count\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.scaler is not None:\n",
    "                self.scaler.scale(total_loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item() if task_count > 0 else 0.0\n",
    "    \n",
    "    def train_epoch(self, tasks, sample_task_batch_fn):\n",
    "        \"\"\"\n",
    "        Run a full training epoch.\n",
    "        \n",
    "        Args:\n",
    "            tasks: List of task dictionaries\n",
    "            sample_task_batch_fn: Function to sample a task batch\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        epoch_losses = []\n",
    "        \n",
    "        n_batches = max(1, len(tasks) // self.cfg.META_BATCH_SIZE)\n",
    "        \n",
    "        pbar = tqdm(range(n_batches), desc=f\"Epoch {self.current_epoch+1}\")\n",
    "        \n",
    "        for batch_idx in pbar:\n",
    "            # Sample tasks\n",
    "            import random\n",
    "            sampled_tasks = random.sample(\n",
    "                tasks,\n",
    "                min(self.cfg.META_BATCH_SIZE, len(tasks))\n",
    "            )\n",
    "            \n",
    "            # Get task batch\n",
    "            task_batch = sample_task_batch_fn(\n",
    "                sampled_tasks,\n",
    "                self.cfg.N_SUPPORT,\n",
    "                self.cfg.N_QUERY\n",
    "            )\n",
    "            \n",
    "            if len(task_batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Train step\n",
    "            loss = self.train_step(task_batch)\n",
    "            epoch_losses.append(loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss:.4f}',\n",
    "                'lr': f'{self.scheduler.get_last_lr()[0]:.6f}'\n",
    "            })\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "        return avg_loss\n",
    "    \n",
    "    def evaluate(self, tasks, sample_task_batch_fn, n_eval=5):\n",
    "        \"\"\"\n",
    "        Evaluate on a handful of validation tasks.\n",
    "        \n",
    "        Args:\n",
    "            tasks: Validation tasks\n",
    "            sample_task_batch_fn: Sampler function\n",
    "            n_eval: Number of tasks to evaluate\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        eval_losses = []\n",
    "        \n",
    "        import random\n",
    "        eval_tasks = random.sample(tasks, min(n_eval, len(tasks)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for task in tqdm(eval_tasks, desc=\"Evaluating\"):\n",
    "                task_batch = sample_task_batch_fn(\n",
    "                    [task],\n",
    "                    self.cfg.N_SUPPORT,\n",
    "                    self.cfg.N_QUERY\n",
    "                )\n",
    "                \n",
    "                if len(task_batch) == 0:\n",
    "                    continue\n",
    "                \n",
    "                task_data = task_batch[0]\n",
    "                support = task_data['support'].to(self.device)\n",
    "                query = task_data['query'].to(self.device)\n",
    "                support_targets = task_data['support_targets'].to(self.device)\n",
    "                query_targets = task_data['query_targets'].to(self.device)\n",
    "                \n",
    "                with torch.amp.autocast('cuda', enabled=self.cfg.MIXED_PRECISION):\n",
    "                    prototype = self.base_model.compute_prototype(support, support_targets)\n",
    "                    loss = self.base_model.compute_loss_with_prototype(\n",
    "                        query, query_targets, prototype\n",
    "                    )\n",
    "                \n",
    "                eval_losses.append(loss.item())\n",
    "        \n",
    "        avg_loss = np.mean(eval_losses) if eval_losses else 0.0\n",
    "        return avg_loss\n",
    "    \n",
    "    def save_checkpoint(self, epoch, path, is_best=False):\n",
    "        \"\"\"\n",
    "        Save a full checkpoint (model + optimizer + scheduler + history).\n",
    "        \n",
    "        Args:\n",
    "            epoch: Current epoch\n",
    "            path: Save path\n",
    "            is_best: Whether this is the best model so far\n",
    "        \"\"\"\n",
    "        # Get base model (kh√¥ng bao g·ªìm DataParallel wrapper)\n",
    "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        \n",
    "        checkpoint = {\n",
    "            # Model weights\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_to_save.state_dict(),\n",
    "            \n",
    "            # Optimizer & Scheduler\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            \n",
    "            # Training history\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            \n",
    "            # Config\n",
    "            'config': {\n",
    "                'META_LR': self.cfg.META_LR,\n",
    "                'IMG_SIZE': self.cfg.IMG_SIZE,\n",
    "                'N_SUPPORT': self.cfg.N_SUPPORT,\n",
    "                'N_QUERY': self.cfg.N_QUERY,\n",
    "            },\n",
    "            \n",
    "            # Scaler state (n·∫øu d√πng mixed precision)\n",
    "            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None,\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, path)\n",
    "        \n",
    "        suffix = \" (BEST)\" if is_best else \"\"\n",
    "        print(f\"‚úì Checkpoint saved: {path}{suffix}\")\n",
    "    \n",
    "    def load_checkpoint(self, path):\n",
    "        \"\"\"\n",
    "        Load a full checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            path: Checkpoint path\n",
    "        \n",
    "        Returns:\n",
    "            epoch: Epoch number\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        # Load model weights\n",
    "        model_to_load = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        model_to_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load optimizer\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Load scheduler\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        # Load training history\n",
    "        self.train_losses = checkpoint.get('train_losses', [])\n",
    "        self.val_losses = checkpoint.get('val_losses', [])\n",
    "        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        \n",
    "        # Load scaler\n",
    "        if self.scaler and checkpoint.get('scaler_state_dict'):\n",
    "            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        \n",
    "        print(f\"‚úì Checkpoint loaded: {path}\")\n",
    "        print(f\"  - Epoch: {checkpoint['epoch']}\")\n",
    "        print(f\"  - Best Val Loss: {self.best_val_loss:.4f}\")\n",
    "        \n",
    "        return checkpoint['epoch']\n",
    "    \n",
    "    def step_scheduler(self):\n",
    "        \"\"\"Update learning rate\"\"\"\n",
    "        self.scheduler.step()\n",
    "    \n",
    "    def get_lr(self):\n",
    "        \"\"\"Get current learning rate\"\"\"\n",
    "        return self.scheduler.get_last_lr()[0]\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "‚úì PrototypicalTrainer ƒê·∫¶Y ƒê·ª¶ CH·ª®C NƒÇNG:\n",
    "  1. ‚úì Multi-GPU support\n",
    "  2. ‚úì Mixed precision training\n",
    "  3. ‚úì Gradient clipping\n",
    "  4. ‚úì Learning rate scheduling\n",
    "  5. ‚úì Save/Load checkpoint ƒê·∫¶Y ƒê·ª¶\n",
    "  6. ‚úì Training history tracking\n",
    "  7. ‚úì Memory optimization\n",
    "  \n",
    "BACKBONE ƒê∆Ø·ª¢C L∆ØU TRONG:\n",
    "- checkpoint['model_state_dict'] ch·ª©a T·∫§T C·∫¢ weights (backbone + heads)\n",
    "- Backbone v·∫´n frozen (kh√¥ng update), nh∆∞ng ƒê∆Ø·ª¢C L∆ØU HO√ÄN CH·ªàNH\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T06:46:09.737734Z",
     "iopub.status.busy": "2025-11-18T06:46:09.737136Z",
     "iopub.status.idle": "2025-11-18T06:46:09.749216Z",
     "shell.execute_reply": "2025-11-18T06:46:09.748640Z",
     "shell.execute_reply.started": "2025-11-18T06:46:09.737716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATASET_DIR = '/kaggle/input/raw-data/train'\n",
    "    OUTPUT_DIR = '/kaggle/working/yolomaml_output'\n",
    "    \n",
    "    # Model\n",
    "    BASE_MODEL = 'yolo11n.pt'\n",
    "    IMG_SIZE = 640\n",
    "    \n",
    "    # Meta-Learning Parameters\n",
    "    META_BATCH_SIZE = 2\n",
    "    N_SUPPORT = 3\n",
    "    N_QUERY = 5\n",
    "    \n",
    "    # Training\n",
    "    META_EPOCHS = 50\n",
    "    INNER_STEPS = 3\n",
    "    META_LR = 0.0005\n",
    "    INNER_LR = 0.01\n",
    "    \n",
    "    # Data split\n",
    "    META_TRAIN_RATIO = 0.8\n",
    "    \n",
    "    # Device\n",
    "    USE_MULTI_GPU = True\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Checkpointing\n",
    "    SAVE_FREQ = 5\n",
    "    \n",
    "    # Memory optimization\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    MIXED_PRECISION = True\n",
    "    \n",
    "cfg = Config()\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T06:46:09.750289Z",
     "iopub.status.busy": "2025-11-18T06:46:09.750038Z",
     "iopub.status.idle": "2025-11-18T06:46:09.768195Z",
     "shell.execute_reply": "2025-11-18T06:46:09.767427Z",
     "shell.execute_reply.started": "2025-11-18T06:46:09.750272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def few_shot_inference(model_path, test_video_dir, output_json='predictions.json', confidence=0.05):\n",
    "    print(\"=\"*80)\n",
    "    print(\"YOLOMAML Few-Shot Inference\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"\\n[1/4] Loading model...\")\n",
    "    model = PrototypicalYOLO(cfg.BASE_MODEL, cfg.DEVICE)\n",
    "    checkpoint = torch.load(model_path, map_location=cfg.DEVICE, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Find videos\n",
    "    print(\"\\n[2/4] Finding test videos...\")\n",
    "    video_folders = sorted([\n",
    "        f for f in os.listdir(test_video_dir)\n",
    "        if os.path.isdir(os.path.join(test_video_dir, f))\n",
    "    ])\n",
    "    print(f\"Found {len(video_folders)} test videos\")\n",
    "    \n",
    "    # Process videos\n",
    "    print(\"\\n[3/4] Processing videos...\")\n",
    "    all_predictions = []\n",
    "    \n",
    "    for video_id in tqdm(video_folders, desc=\"Inference\"):\n",
    "        video_dir = os.path.join(test_video_dir, video_id)\n",
    "        video_path = os.path.join(video_dir, 'drone_video.mp4')\n",
    "        ref_dir = os.path.join(video_dir, 'object_images')\n",
    "        \n",
    "        # Check video path\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"\\n‚ö†Ô∏è  Video not found: {video_path}\")\n",
    "            all_predictions.append({\"video_id\": video_id, \"annotations\": []})\n",
    "            continue\n",
    "        \n",
    "        # Load support images\n",
    "        support_images = []\n",
    "        for img_name in ['img_1.jpg', 'img_2.jpg', 'img_3.jpg']:\n",
    "            img_path = os.path.join(ref_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                img = load_and_preprocess_image(img_path, cfg.IMG_SIZE)\n",
    "                if img is not None:\n",
    "                    support_images.append(img)\n",
    "        \n",
    "        # Check support images\n",
    "        if len(support_images) < 3:\n",
    "            print(f\"\\n‚ö†Ô∏è  Not enough reference images for {video_id}: {len(support_images)}/3\")\n",
    "            all_predictions.append({\"video_id\": video_id, \"annotations\": []})\n",
    "            continue\n",
    "        \n",
    "        support_tensor = torch.stack(support_images).to(cfg.DEVICE)\n",
    "        \n",
    "        # Create support targets\n",
    "        support_targets = torch.tensor([\n",
    "            [0.5, 0.5, 0.4, 0.4]\n",
    "        ]).repeat(len(support_images), 1).to(cfg.DEVICE)\n",
    "        \n",
    "        # Compute prototype\n",
    "        with torch.no_grad():\n",
    "            prototype = model.compute_prototype(support_tensor, support_targets)\n",
    "        \n",
    "        # Process video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_idx = 0\n",
    "        video_bboxes = []\n",
    "        \n",
    "        orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        detection_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Preprocess\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_resized = cv2.resize(frame_rgb, (cfg.IMG_SIZE, cfg.IMG_SIZE))\n",
    "                frame_tensor = torch.from_numpy(frame_resized).permute(2, 0, 1).float() / 255.0\n",
    "                frame_tensor = frame_tensor.unsqueeze(0).to(cfg.DEVICE)\n",
    "                \n",
    "                # Get predictions\n",
    "                bbox_pred, obj_pred = model(frame_tensor)\n",
    "                embeddings = model.forward(frame_tensor, return_embeddings=True)\n",
    "                \n",
    "                B, C, H, W = embeddings.shape\n",
    "                bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(B, -1, 4)\n",
    "                obj_pred = obj_pred.permute(0, 2, 3, 1).reshape(B, -1)\n",
    "                embeddings = embeddings.permute(0, 2, 3, 1).reshape(B, -1, C)\n",
    "                \n",
    "                # Compute distances\n",
    "                prototype_expanded = prototype.unsqueeze(0).unsqueeze(0)\n",
    "                distances = torch.norm(embeddings - prototype_expanded, dim=2)\n",
    "                \n",
    "                # Get TOP-K anchors\n",
    "                TOP_K = 10\n",
    "                topk_vals, topk_indices = torch.topk(distances[0], k=TOP_K, largest=False)\n",
    "                \n",
    "                for k_idx in range(TOP_K):\n",
    "                    anchor_idx = topk_indices[k_idx].item()\n",
    "                    obj_score = torch.sigmoid(obj_pred[0, anchor_idx]).item()\n",
    "                    \n",
    "                    if obj_score > confidence:\n",
    "                        best_bbox = bbox_pred[0, anchor_idx].cpu().numpy()\n",
    "                        \n",
    "                        # Convert to pixel coords\n",
    "                        x_c, y_c, w, h = best_bbox\n",
    "                        x_c_pixel = x_c * orig_width\n",
    "                        y_c_pixel = y_c * orig_height\n",
    "                        w_pixel = w * orig_width\n",
    "                        h_pixel = h * orig_height\n",
    "                        \n",
    "                        x1 = int(x_c_pixel - w_pixel/2)\n",
    "                        y1 = int(y_c_pixel - h_pixel/2)\n",
    "                        x2 = int(x_c_pixel + w_pixel/2)\n",
    "                        y2 = int(y_c_pixel + h_pixel/2)\n",
    "                        \n",
    "                        # Clamp\n",
    "                        x1 = max(0, min(x1, orig_width))\n",
    "                        y1 = max(0, min(y1, orig_height))\n",
    "                        x2 = max(0, min(x2, orig_width))\n",
    "                        y2 = max(0, min(y2, orig_height))\n",
    "                        \n",
    "                        # Valid box check\n",
    "                        if x2 > x1 and y2 > y1:\n",
    "                            video_bboxes.append({\n",
    "                                \"frame\": frame_idx,\n",
    "                                \"x1\": x1, \"y1\": y1,\n",
    "                                \"x2\": x2, \"y2\": y2\n",
    "                            })\n",
    "                            detection_count += 1\n",
    "                            break\n",
    "                \n",
    "                frame_idx += 1\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Log summary for this video\n",
    "        print(f\"\\nüìπ {video_id}: {detection_count}/{total_frames} detections\")\n",
    "        \n",
    "        # Log random sample of 5 bboxes (if available)\n",
    "        if len(video_bboxes) > 0:\n",
    "            sample_size = min(5, len(video_bboxes))\n",
    "            sample_bboxes = random.sample(video_bboxes, sample_size)\n",
    "            print(f\"   Sample detections:\")\n",
    "            for bbox in sample_bboxes:\n",
    "                print(f\"   - Frame {bbox['frame']:3d}: [{bbox['x1']:4d},{bbox['y1']:4d},{bbox['x2']:4d},{bbox['y2']:4d}]\")\n",
    "        \n",
    "        # Format annotations\n",
    "        annotations = []\n",
    "        if len(video_bboxes) > 0:\n",
    "            annotations.append({\"bboxes\": video_bboxes})\n",
    "        \n",
    "        all_predictions.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"annotations\": annotations\n",
    "        })\n",
    "    \n",
    "    # Save\n",
    "    print(f\"\\n[4/4] Saving predictions to {output_json}...\")\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(all_predictions, f, indent=4)\n",
    "    \n",
    "    print(\"\\n‚úì Inference complete!\")\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T06:46:09.769225Z",
     "iopub.status.busy": "2025-11-18T06:46:09.768995Z",
     "iopub.status.idle": "2025-11-18T06:46:09.783126Z",
     "shell.execute_reply": "2025-11-18T06:46:09.782380Z",
     "shell.execute_reply.started": "2025-11-18T06:46:09.769208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_complete_inference():\n",
    "    \"\"\"Run complete inference pipeline\"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    MODEL_PATH = '/kaggle/input/prototypical-model/prototypical_best.pt'\n",
    "    TEST_DIR = '/kaggle/input/public-test/public_test/samples'\n",
    "    OUTPUT_JSON = 'yolomaml_predictions.json'\n",
    "    \n",
    "    # Check if model exists\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Model not found at {MODEL_PATH}\")\n",
    "        print(\"Please train the model first!\")\n",
    "        return\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"\\nRunning Few-Shot Inference...\")\n",
    "    predictions = few_shot_inference(\n",
    "        model_path=MODEL_PATH,\n",
    "        test_video_dir=TEST_DIR,\n",
    "        output_json=OUTPUT_JSON,\n",
    "        confidence=0.05\n",
    "    )\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INFERENCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úì Predictions saved to: {OUTPUT_JSON}\")\n",
    "    print(f\"‚úì Total videos processed: {len(predictions)}\")\n",
    "    \n",
    "    # Count detections\n",
    "    total_detections = sum(\n",
    "        len(ann['bboxes']) \n",
    "        for v in predictions \n",
    "        for ann in v['annotations']\n",
    "    )\n",
    "    print(f\"‚úì Total detections: {total_detections}\")\n",
    "    \n",
    "    # Per-video breakdown\n",
    "    print(f\"\\nPer-video detections:\")\n",
    "    for v in predictions:\n",
    "        det_count = sum(len(ann['bboxes']) for ann in v['annotations'])\n",
    "        print(f\"  ‚Ä¢ {v['video_id']}: {det_count} detections\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T06:46:09.784049Z",
     "iopub.status.busy": "2025-11-18T06:46:09.783832Z",
     "iopub.status.idle": "2025-11-18T07:08:47.250719Z",
     "shell.execute_reply": "2025-11-18T07:08:47.249869Z",
     "shell.execute_reply.started": "2025-11-18T06:46:09.784033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Few-Shot Inference...\n",
      "================================================================================\n",
      "YOLOMAML Few-Shot Inference\n",
      "================================================================================\n",
      "\n",
      "[1/4] Loading model...\n",
      "\n",
      "[2/4] Finding test videos...\n",
      "Found 6 test videos\n",
      "\n",
      "[3/4] Processing videos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74af993c81524dac9154e3d0456c4a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìπ BlackBox_0: 708/5443 detections\n",
      "   Sample detections:\n",
      "   - Frame 5371: [ 427, 278, 475, 293]\n",
      "   - Frame 860: [ 431, 282, 475, 292]\n",
      "   - Frame 369: [ 453, 259, 488, 283]\n",
      "   - Frame 2488: [ 494, 316, 522, 335]\n",
      "   - Frame 2167: [ 498, 317, 524, 332]\n",
      "\n",
      "üìπ BlackBox_1: 666/5776 detections\n",
      "   Sample detections:\n",
      "   - Frame 1989: [ 489, 304, 519, 315]\n",
      "   - Frame 1071: [ 434, 258, 473, 274]\n",
      "   - Frame 905: [ 458, 272, 500, 287]\n",
      "   - Frame 5008: [ 492, 307, 535, 321]\n",
      "   - Frame 840: [ 524, 336, 556, 354]\n",
      "\n",
      "üìπ CardboardBox_0: 554/5285 detections\n",
      "   Sample detections:\n",
      "   - Frame 3905: [ 410, 237, 434, 258]\n",
      "   - Frame 480: [ 483, 315, 506, 333]\n",
      "   - Frame 475: [ 463, 277, 478, 292]\n",
      "   - Frame 1131: [ 479, 275, 511, 295]\n",
      "   - Frame 2327: [ 444, 302, 492, 318]\n",
      "\n",
      "üìπ CardboardBox_1: 700/5942 detections\n",
      "   Sample detections:\n",
      "   - Frame 1616: [ 449, 278, 474, 295]\n",
      "   - Frame 2884: [ 475, 293, 483, 314]\n",
      "   - Frame 4957: [ 439, 250, 468, 268]\n",
      "   - Frame 363: [ 470, 309, 507, 322]\n",
      "   - Frame 4581: [ 476, 321, 530, 345]\n",
      "\n",
      "üìπ LifeJacket_0: 2013/8309 detections\n",
      "   Sample detections:\n",
      "   - Frame 2153: [ 440, 251, 455, 268]\n",
      "   - Frame 5749: [ 484, 305, 513, 324]\n",
      "   - Frame 5294: [ 450, 306, 489, 328]\n",
      "   - Frame 5543: [ 452, 301, 484, 318]\n",
      "   - Frame 4098: [ 445, 274, 472, 290]\n",
      "\n",
      "üìπ LifeJacket_1: 651/4820 detections\n",
      "   Sample detections:\n",
      "   - Frame 3146: [ 469, 248, 504, 271]\n",
      "   - Frame 2207: [ 453, 279, 479, 296]\n",
      "   - Frame 3488: [ 438, 257, 460, 275]\n",
      "   - Frame 4553: [ 431, 273, 471, 288]\n",
      "   - Frame 2734: [ 458, 285, 496, 305]\n",
      "\n",
      "[4/4] Saving predictions to yolomaml_predictions.json...\n",
      "\n",
      "‚úì Inference complete!\n",
      "\n",
      "================================================================================\n",
      "INFERENCE SUMMARY\n",
      "================================================================================\n",
      "‚úì Predictions saved to: yolomaml_predictions.json\n",
      "‚úì Total videos processed: 6\n",
      "‚úì Total detections: 5292\n",
      "\n",
      "Per-video detections:\n",
      "  ‚Ä¢ BlackBox_0: 708 detections\n",
      "  ‚Ä¢ BlackBox_1: 666 detections\n",
      "  ‚Ä¢ CardboardBox_0: 554 detections\n",
      "  ‚Ä¢ CardboardBox_1: 700 detections\n",
      "  ‚Ä¢ LifeJacket_0: 2013 detections\n",
      "  ‚Ä¢ LifeJacket_1: 651 detections\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "predictions = run_complete_inference()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8763129,
     "sourceId": 13769014,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8766428,
     "sourceId": 13773696,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
